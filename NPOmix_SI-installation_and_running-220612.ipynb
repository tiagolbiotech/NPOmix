{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machines used\n",
    "\n",
    "### MacBook Pro Mojave OS\n",
    "\n",
    "Most tools (except antiSMASH, Nerpa and the ones with webservers) were executed in the MacBook Pro with:\n",
    "\n",
    "```\n",
    "Processor: 2.4 GHz Intel Core i5, 8 CPUs.\n",
    "Memory: 8 GB 2133 MHz LPDDR3.\n",
    "```\n",
    "\n",
    "### Server\n",
    "\n",
    "AntiSMASH and Nerpa were executed at a server node with:\n",
    "\n",
    "```\n",
    "Processor: Intel Xeon X7560 2.27 GHz, 16 CPUs.\n",
    "Memory: ~13 GB RAM is more than enough for up to 100 genomes, if thousands genomes are needed, use ~20 GB.\n",
    "```\n",
    "\n",
    "# Installation\n",
    "\n",
    "### Anaconda\n",
    "\n",
    "We used the command-line install according to `https://docs.anaconda.com/anaconda/install/mac-os/`.\n",
    "\n",
    "### Docker\n",
    "\n",
    "We installed the docker app from `https://docs.docker.com/desktop/mac/install/`.\n",
    "\n",
    "### antiSMASH\n",
    "\n",
    "AntiSMASH can be installed using Anaconda:\n",
    "\n",
    "`conda install -c bioconda antismash`\n",
    "\n",
    "### BiG-SCAPE\n",
    "\n",
    "1. Download pFAM here:\n",
    "\n",
    "`wget http://ftp.ebi.ac.uk/pub/databases/Pfam/releases/Pfam35.0/Pfam-A.hmm.gz`\n",
    "\n",
    "2. Install software:\n",
    "\n",
    "`conda create -n bigscape -c bioconda bigscape`\n",
    "\n",
    "### SIRIUS/CANOPUS\n",
    "\n",
    "Download th SIRIUS app from `https://boecker-lab.github.io/docs.sirius.github.io/install/#mac-osx`.\n",
    "\n",
    "`git clone https://github.com/kaibioinfo/canopus_treemap.git`\n",
    "\n",
    "### NPOmix\n",
    "\n",
    "`1.` Clone the repository with `git clone https://github.com/tiagolbiotech/NPOmix_python`. \n",
    "\n",
    "PS: You can also use the jupyter notebook version but it is not as updated as the python version. \n",
    "\n",
    "For such, use `git clone https://github.com/tiagolbiotech/NPOmix.git`\n",
    "\n",
    "`2.` Download PoDP database\n",
    "\n",
    "Please use the notebook `NPOmix_v1.0-notebook1_downloading-210811.ipynb` to download and parse the data. Or, you can download data ready to run using the Zenodo links at the end of this document.\n",
    "\n",
    "`3.` Install the following python packages`\n",
    "\n",
    "`conda install -c bioconda pyteomics`\n",
    "\n",
    "`conda install -c anaconda requests`\n",
    "\n",
    "`conda install -c anaconda networkx`\n",
    "\n",
    "`conda install -c trentonoliphant datetime`\n",
    "\n",
    "`conda install -c conda-forge biopython`\n",
    "\n",
    "`conda install -c anaconda scikit-learn`\n",
    "\n",
    "### Nerpa\n",
    "\n",
    "`\n",
    "conda install -c bioconda nerpa\n",
    "`\n",
    "\n",
    "### NPLinker\n",
    "\n",
    "`\n",
    "docker pull andrewramsay/nplinker:mibigtest\n",
    "`\n",
    "\n",
    "### NRPminer\n",
    "\n",
    "NRPminer is not publically availible yet and we were able to use it through our collaboration with Dr Alexey Gurevich, one of the developers.\n",
    "\n",
    "### Other tools\n",
    "\n",
    "The following tools can be used online: Metaminer [(link here)](https://gnps.ucsd.edu/ProteoSAFe/index.jsp?params=%7B%22workflow%22:%22RIPPQUEST%22%7D); DeepRiPP [(link here)](http://deepripp.magarveylab.ca).\n",
    "\n",
    "The following tools seem to be discontinued: GARLIC; GNP; NRPquest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running softwares\n",
    "\n",
    "### antiSMASH\n",
    "\n",
    "`antismash XXX`\n",
    "\n",
    "\n",
    "### BiG-SCAPE\n",
    "\n",
    "\n",
    "`conda activate bigscape`\n",
    "\n",
    "`bigscape.py --pfam_dir /path/to/pfam_files/ -i /path/to/antismash/ \\\n",
    "-o /path/to/bigscape_outputs_220603_1791samples/ -c 8 --include_singletons --mibig`\n",
    "\n",
    "### SIRIUS/CANOPUS\n",
    "\n",
    "\n",
    "`cd /path/to/canopus_treemap/`\n",
    "\n",
    "`./sirius.app/Contents/MacOS/sirius -i /path/to/all_mzml/ -o sirius_npomix --maxmz=800 lcms-align sirius zodiac fingerid canopus`\n",
    "\n",
    "`./sirius.app/Contents/MacOS/sirius -i sirius_npomix mgf-export --merge-ms2  --quant-table=colabA-quant.csv --output colabA.mgf`\n",
    "\n",
    "### NPOmix python version\n",
    "\n",
    "1. Open file at \n",
    "\n",
    "`/path/to/NPOmix_python/npomix.py` (this path is where you cloned the folder)\n",
    "\n",
    "2. Modify paths properly. Replace `/path/to/` with your actual path to the proper folder.\n",
    "\n",
    "```\n",
    "mgf_folder = \"/path/to/selected_mgf/\" # this folder needs to contain all mgf files (MS/MS data) to be tested, they can be generated by the SIRIUS commands above\n",
    "LCMS_folder = \"/path/to/podp_LCMS_round5/\" # this folder needs to contain all mz(X)ML files in the training set\n",
    "ena_df_file = \"/path/to/NPOmix_python/ena_dict-210315.csv\" # this file contains the correspondance between ENA codes\n",
    "input_bigscape_net = \"/path/to/bigscape_all_c030_220512_1777samples.txt\" # this file contains the BiG-SCAPE scores for all BGCs from the all genomes (in the paper we used 1,040 genomes)\n",
    "antismash_folder = \"/path/to/antismash/\" # this folder needs to contain all antismash files (annotated genomes) to be used in the training set\n",
    "merged_ispec_mat_file = \"/path/to/NPOmix_python/mass-affinity_df-NPOmix1.0_rene-%s.txt\"%current_date # (OPTIONAL) if you already ran the step to obtain the merged_ispec_mat, you can skip this time consuming step by inputting this file\n",
    "results_folder = \"/path/to/NPOmix_python/main_code_results/\" # folder where the results will be saved\n",
    "k_value = 3\n",
    "```\n",
    "\n",
    "3. Run the tool:\n",
    "\n",
    "`python npomix.py`\n",
    "\n",
    "### NPOmix website submission\n",
    "\n",
    "Go to `https://www.tfleao.com/general-8` and submit your samples. \n",
    "\n",
    "This website also contains video tutorials and workshops (in Portuguese and English).\n",
    "\n",
    "### Nerpa\n",
    "\n",
    "```\n",
    "# for the run against the default Nerpa database of  8,368 known and putative NRP structures from the Nerpa paper supplement at https://zenodo.org/record/5503984 (the full metadata is also there in pnrpdb_summary.tsv)\n",
    "wget https://zenodo.org/record/5503984/files/pnrpdb_preprocessed.info   # downloading the preprocessed database\n",
    "nerpa.py --threads $THREADS -a $ANTISMASH_PROCESSED_INPUT --structures pnrpdb_preprocessed.info --process-hybrids -o $OUTPUT_DIR\n",
    "\n",
    "# for the run against specific set of structures, e.g., the selected 22 metabolites (in the SMILES format)\n",
    "nerpa.py --threads $THREADS -a $ANTISMASH_PROCESSED_INPUT --smiles-tsv $SMILES_INPUT --col-smiles \"SMILES\" --col-id \"Accession\" --process-hybrids -o $OUTPUT_DIR\n",
    "\n",
    "Where \n",
    "$THREADS - the number of threads (int)\n",
    "$ANTISMASH_PROCESSED_INPUT - the path to a directory with antiSMASH output or to a directory with multiple antiSMASH outputs, e.g., results for a set of genomes. Supported antiSMASH versions are v.3, v.5, and v.6.\n",
    "$SMILES_INPUT - the path to a tab-separated file with structures in the SMILES format, might also include compound names (in this example, I assume the names are in the Accession column).\n",
    "\n",
    "An example $SMILES_INPUT is attached.\n",
    "An example content of an $ANTISMASH_PROCESSED_INPUT is below (the key file Nerpa actually uses is <dirname>.json)\n",
    "\n",
    "ls ./antiSMASH/GCA_000012265/\n",
    "CP000076.1.region001.gbk  CP000076.1.region005.gbk  CP000076.1.region009.gbk  CP000076.1.region013.gbk  GCA_000012265.gbk   images             knownclusterblastoutput.txt\n",
    "CP000076.1.region002.gbk  CP000076.1.region006.gbk  CP000076.1.region010.gbk  CP000076.1.region014.gbk  GCA_000012265.json  index.html         regions.js\n",
    "CP000076.1.region003.gbk  CP000076.1.region007.gbk  CP000076.1.region011.gbk  CP000076.1.region015.gbk  GCA_000012265.zip   js                 svg\n",
    "CP000076.1.region004.gbk  CP000076.1.region008.gbk  CP000076.1.region012.gbk  CP000076.1.region016.gbk  css                 knownclusterblast\n",
    "```\n",
    "\n",
    "### NPLinker\n",
    "\n",
    "We prepared the folder with the data according to `https://github.com/NPLinker/nplinker/wiki/LoadingLocalData`. \n",
    "\n",
    "We used the Docker version with the following commands:\n",
    "\n",
    "`docker run --name webapp -p 5006:5006 -v c:/Users/myusername/nplinker_shared:/data:rw andrewramsay/nplinker:mibigtest`\n",
    "\n",
    "Then we visualized the results in the browser using the url `http://localhost:5006/npapp`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloaded NPOmix database\n",
    "\n",
    "antiSMASH folders (BGCs only) are availible at: `https://doi.org/10.5281/zenodo.6637083`.\n",
    "\n",
    "LCMS metabolomes (MGF only) will be availible soon, we need to extract the MGFs from the mzML first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
